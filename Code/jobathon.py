# -*- coding: utf-8 -*-
"""Jobathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18BCDP7pWZV0cSuTI_jqOpYhy83P80Bj0

# Data Preprocessing
"""

import pandas as pd
import numpy as np

df = pd.read_csv('train.csv')
print(df.shape)
df.head()

len(df['Emp_ID'].unique())

"""There are only 2381 employees, but we have total 19104 records. That means we have multiple records for the same emplyee.
we will keep the latest record only which is required to determine whether an employee is currently working or not.
"""

df.drop_duplicates(subset=['Emp_ID'], keep='last', inplace=True)
print(df.shape)
df.head()

df.reset_index(drop=True, inplace=True)
print(df.shape)
df

df.isnull().sum()

"""Getting the target variable from LastWorkingDate column."""

df['LastWorkingDate'].fillna(0, inplace=True)

df['target'] = df['LastWorkingDate'].apply(lambda x: 0 if x==0 else 1)
df.head()

"""Removing unnecessary columns"""

df = df.drop(labels=['MMM-YY', 'Dateofjoining', 'LastWorkingDate'], axis=1)
df.head()

df.target.value_counts()

"""Handling categorical values."""

df['Gender'] = df['Gender'].apply(lambda x: 1 if x=='Male' else 0)
df['Education_Level'] = df['Education_Level'].apply(lambda x: 1 if x=='College' else 2 if x=='Bachelor' else 3)

df_dummies = pd.get_dummies(data=df.City)
df_dummies

df_with_dummies = pd.concat([df.drop(labels=['City'], axis=1), df_dummies], axis=1)
df_with_dummies

"""Standardization"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
df_with_dummies[['Age', 'Salary', 'Education_Level', 'Joining Designation', 'Designation', 'Total Business Value', 'Quarterly Rating']] = sc.fit_transform(df_with_dummies[['Age', 'Salary', 'Education_Level', 'Joining Designation', 'Designation', 'Total Business Value', 'Quarterly Rating']])
df_with_dummies

"""Since we have to predict on the train data itself we will not split it into train-test data."""

X = df_with_dummies.drop(labels=['Emp_ID', 'target'], axis=1)
y = df_with_dummies['target']
X.shape, y.shape

"""#Training different models

##model-1 XGBClassifier
"""

from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score

model_xg = XGBClassifier(n_estimators=1000, learning_rate=0.05)
model_xg.fit(X, y, early_stopping_rounds=5, 
             eval_set=[(X_test, y_test)], verbose=False)
y_pred_xg = model_xg.predict(X)

print(confusion_matrix(y_pred_xg, y))
print(accuracy_score(y_pred_xg, y))
print(f1_score(y_pred_xg, y))

"""##model-2 XGBClassifier with RandomizedSearchCV"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from xgboost import XGBClassifier

model_XGB_cv = XGBClassifier() 
params={
 "n_estimators"     : [250, 500, 1000, 750, 800, 1500, 2000],
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
    
}
random_search=RandomizedSearchCV(model_XGB_cv,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)
random_search.fit(X, y)
random_search.best_estimator_

model_XGB_CV = XGBClassifier(colsample_bytree=0.7, gamma=0.0, learning_rate=0.05, max_depth=4,
              n_estimators=250)

model_XGB_CV.fit(X, y, early_stopping_rounds=5, 
             eval_set=[(X_test, y_test)], verbose=False)
y_pred_xg = model_XGB_CV.predict(X)

print(confusion_matrix(y_pred_xg, y))
print(accuracy_score(y_pred_xg, y))
print(f1_score(y_pred_xg, y))

"""## model-3 Kernel SVC"""

from sklearn.svm import SVC
model_svc_2 = SVC(kernel='rbf')
model_svc_2.fit(X, y)

y_pred_svc_2 = model_svc_2.predict(X)

print(confusion_matrix(y_pred_svc_2, y))
print(accuracy_score(y_pred_svc_2, y))
print(f1_score(y_pred_svc_2, y))

"""## model-4 SVC with GridSearchCV"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
model_svc = SVC()
parameters = {
    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],
    'gamma':['scale', 'auto'],
    'C':[0.2, 0.4, 0.6, 0.8, 1.0]
}
cv = GridSearchCV(estimator=model_svc, param_grid=parameters)
cv.fit(X, y)
print(cv.best_params_)

from sklearn.svm import SVC
model_svc = SVC(kernel='linear', gamma='scale', C=0.6)
model_svc.fit(X, y)

y_pred_svc = model_svc.predict(X)

print(confusion_matrix(y_pred_svc, y))
print(accuracy_score(y_pred_svc, y))
print(f1_score(y_pred_svc, y))

"""## model-5 KNN with GridSearchCV"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
model_knn = KNeighborsClassifier()
parameters = {
    "n_neighbors" : [5, 10, 15, 20, 25, 30, 35, 40]
}
cv = GridSearchCV(estimator=model_knn, param_grid=parameters)
cv.fit(X, y)
print(cv.best_params_)

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
model_knn = KNeighborsClassifier(n_neighbors=30)
model_knn.fit(X, y)

y_pred_knn = model_knn.predict(X)

print(confusion_matrix(y_pred_knn, y))
print(accuracy_score(y_pred_knn, y))
print(f1_score(y_pred_knn, y))

"""## model-6 RandomForest with GridSearchCV"""

from sklearn.ensemble import RandomForestClassifier
Classifier = RandomForestClassifier()
parameters = {
    "n_estimators":[5,10,50,100,250],
    "max_depth":[2,4,8,16,32,None], 
    "criterion": ['gini', 'entropy'],
}
cv = GridSearchCV(estimator=Classifier, param_grid=parameters)
cv.fit(X, y)
print(cv.best_params_)

model_forest = RandomForestClassifier(n_estimators=250, max_depth=4, criterion='gini')
model_forest.fit(X, y)

y_pred_rand = model_forest.predict(X)

print(confusion_matrix(y_pred_rand, y))
print(accuracy_score(y_pred_rand, y))
print(f1_score(y_pred_rand, y))

"""## model-7 GaussianNB"""

from sklearn.naive_bayes import GaussianNB
model_bayes = GaussianNB()
model_bayes.fit(X, y)

y_pred_gaus = model_bayes.predict(X)

print(confusion_matrix(y_pred_gaus, y))
print(accuracy_score(y_pred_gaus, y))
print(f1_score(y_pred_gaus, y))

"""## Ensembel model"""

#function which will count the main prediction using all the models
def get_main(DF, models, model_names, X):
  for i, column in enumerate(DF.columns):
    DF[column] = models[i].predict(X)

  DF['main_prediction'] = ''

  for i in range(len(DF)):
    count_1 = 0
    count_0 = 0
    for col in range(len(DF.columns)-1):
      if int(DF.iloc[i, col]) == 1:
        count_1 += 1
      else:
        count_0 += 1
    result = 0 if count_0 >= count_1 else 1
    DF['main_prediction'][i] = result

  return DF

models = [model_xg, model_XGB_CV, model_knn, model_forest, model_bayes, model_svc, model_svc_2]
model_names = ['model_xg', 'model_XGB_CV', 'model_knn', 'model_forest', 'model_bayes', 'model_svc', 'model_svc_2']

df_to_get_main_1 = pd.DataFrame(columns = model_names)
df_main_1 = get_main(df_to_get_main_1, models, model_names, X)

df_main_1

"""#Preparing the submission.csv"""

df_test = pd.read_csv('test.csv')
df_test

emp_list = list(df_test.Emp_ID.unique())

column_names = df_with_dummies.columns
df_test_to_new = pd.DataFrame(columns = column_names)

for i in emp_list:
    df_test_to_new = df_test_to_new.append(df_with_dummies[df_with_dummies['Emp_ID'] == i] , ignore_index = True)
df_test_to_new

##X = df_with_dummies.drop(labels=['Emp_ID', 'target'], axis=1)

X_new = df_test_to_new.drop(labels=['Emp_ID', 'target'], axis=1)
X_new.shape

X_new = X_new.astype(np.float32)

models = [model_xg, model_XGB_CV, model_knn, model_forest, model_bayes, model_svc, model_svc_2]
model_names = ['model_xg', 'model_XGB_CV', 'model_knn', 'model_forest', 'model_bayes', 'model_svc', 'model_svc_2']

df_to_get_main_2 = pd.DataFrame(columns = model_names)
df_main_2 = get_main(df_to_get_main_2, models, model_names, X_new)
df_main_2

y_pred = df_main_2.main.values
sub = pd.DataFrame(data = y_pred, 
                  columns = ['predicted'])
sub

sub.predicted.value_counts()

df_final = pd.concat([df_test_to_new, sub], axis=1)
df_final

df_final = df_final[['Emp_ID', 'predicted']]
df_final

df_final.to_csv('sub_8.csv')